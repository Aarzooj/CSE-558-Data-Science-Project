{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":65711,"databundleVersionId":7405009,"sourceType":"competition"},{"sourceId":9789868,"sourceType":"datasetVersion","datasetId":5998716}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import HashingVectorizer\n\ndef apply_feature_hashing(df, categorical_columns, n_features=5):\n    hasher = HashingVectorizer(n_features=n_features, norm=None, alternate_sign=False)\n    for col in categorical_columns:\n        # Ensure the column is treated as string\n        df[col] = df[col].astype(str)\n        # Apply feature hashing\n        hashed_features = hasher.transform(df[col]).toarray()\n        # Create hashed feature column names\n        hashed_columns = [f\"{col}_hash_{i}\" for i in range(n_features)]\n        hashed_df = pd.DataFrame(hashed_features, columns=hashed_columns, index=df.index)\n        # Replace the original column with hashed features\n        df = pd.concat([df.drop(columns=[col]), hashed_df], axis=1)\n    return df\n\ncategorical_columns = ['Surname', 'Geography', 'Gender']\n\n\nhashed_df = apply_feature_hashing(df, categorical_columns, n_features=5)\n\nprint(hashed_df.head())\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T23:10:21.457033Z","iopub.execute_input":"2024-12-02T23:10:21.457653Z","iopub.status.idle":"2024-12-02T23:10:23.320015Z","shell.execute_reply.started":"2024-12-02T23:10:21.457608Z","shell.execute_reply":"2024-12-02T23:10:23.319133Z"}},"outputs":[{"name":"stdout","text":"    id  CustomerId  CreditScore   Age  Tenure    Balance  NumOfProducts  \\\n0  0.0  15674932.0        668.0  33.0     3.0       0.00            2.0   \n1  1.0  15749177.0        627.0  33.0     1.0       0.00            2.0   \n2  2.0  15694510.0        678.0  40.0    10.0       0.00            2.0   \n3  3.0  15741417.0        581.0  34.0     2.0  148882.54            1.0   \n4  4.0  15766172.0        716.0  33.0     5.0       0.00            2.0   \n\n   HasCrCard  IsActiveMember  EstimatedSalary  ...  Geography_hash_0  \\\n0        1.0             0.0        181449.97  ...               0.0   \n1        1.0             1.0         49503.50  ...               0.0   \n2        1.0             0.0        184866.69  ...               0.0   \n3        1.0             1.0         84560.88  ...               0.0   \n4        1.0             1.0         15068.83  ...               0.0   \n\n   Geography_hash_1  Geography_hash_2  Geography_hash_3  Geography_hash_4  \\\n0               0.0               0.0               1.0               0.0   \n1               0.0               0.0               1.0               0.0   \n2               0.0               0.0               1.0               0.0   \n3               0.0               0.0               1.0               0.0   \n4               0.0               0.0               0.0               1.0   \n\n   Gender_hash_0  Gender_hash_1  Gender_hash_2  Gender_hash_3  Gender_hash_4  \n0            0.0            0.0            0.0            0.0            1.0  \n1            0.0            0.0            0.0            0.0            1.0  \n2            0.0            0.0            0.0            0.0            1.0  \n3            0.0            0.0            0.0            0.0            1.0  \n4            0.0            0.0            0.0            0.0            1.0  \n\n[5 rows x 26 columns]\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction import FeatureHasher\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\ndf = pd.read_csv('/kaggle/input/complete-dataset/augmented_data.csv')\ndf = df.drop_duplicates()\n\ndef fill_missing_values(df):\n    for col in df.select_dtypes(include=['float64', 'int64']).columns:\n        df[col] = df[col].fillna(df[col].mean())\n    for col in df.select_dtypes(include=['object']).columns:\n        df[col] = df[col].fillna(df[col].mode()[0])\n    return df\n\ndf = fill_missing_values(df)\n\ncolumns_to_drop = ['id', 'CustomerId']\ndf = df.drop(columns=columns_to_drop)\n\ndf['Exited'] = df['Exited'].astype(int)\n\nprint(\"Unique values in Exited:\", df['Exited'].unique())\n\nprint(\"Number of NaN values before cleaning:\")\nprint(df.isnull().sum())\n\ndf = df.dropna()\n\nprint(\"\\nNumber of NaN values after cleaning:\")\nprint(df.isnull().sum())\n\ncategorical_columns = ['Surname', 'Geography', 'Gender']\nnumerical_columns = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', \n                    'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n\nhasher = FeatureHasher(n_features=20, input_type='string')\nhashed_features = np.zeros((len(df), 20 * len(categorical_columns)))\n\nfor i, col in enumerate(categorical_columns):\n    col_data = df[col].astype(str).values\n    col_data = col_data.reshape(-1, 1)\n    hashed = hasher.transform(col_data).toarray()\n    start_idx = i * 20\n    end_idx = (i + 1) * 20\n    hashed_features[:, start_idx:end_idx] = hashed\n\nnumerical_features = df[numerical_columns].values\nX = np.hstack((numerical_features, hashed_features))\n\nhashed_column_names = []\nfor cat_col in categorical_columns:\n    for i in range(20):\n        hashed_column_names.append(f'{cat_col}_hash_{i}')\n\ndf_hashed = pd.DataFrame(X, columns=numerical_columns + hashed_column_names)\ndf_hashed['Exited'] = df['Exited']\n\nprint(\"\\nShape of transformed dataframe:\", df_hashed.shape)\nprint(\"\\nFirst few rows of the transformed dataframe:\")\nprint(df_hashed.head())\n\ny = df_hashed['Exited'].values.astype(int)  # Ensure target is integer\nX = df_hashed.drop('Exited', axis=1).values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Train and evaluate models\n# SVM\nprint(\"\\nTraining SVM...\")\nsvm = SVC(kernel='rbf', random_state=42)\nsvm.fit(X_train_scaled, y_train)\nsvm_pred = svm.predict(X_test_scaled)\nprint(\"\\nSVM Results:\")\nprint(\"Accuracy:\", accuracy_score(y_test, svm_pred))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, svm_pred))\n\n# Random Forest\nprint(\"\\nTraining Random Forest...\")\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train_scaled, y_train)\nrf_pred = rf.predict(X_test_scaled)\nprint(\"\\nRandom Forest Results:\")\nprint(\"Accuracy:\", accuracy_score(y_test, rf_pred))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, rf_pred))\n\n# Logistic Regression\nprint(\"\\nTraining Logistic Regression...\")\nlr = LogisticRegression(random_state=42, max_iter=1000)\nlr.fit(X_train_scaled, y_train)\nlr_pred = lr.predict(X_test_scaled)\nprint(\"\\nLogistic Regression Results:\")\nprint(\"Accuracy:\", accuracy_score(y_test, lr_pred))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, lr_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T23:10:23.321504Z","iopub.execute_input":"2024-12-02T23:10:23.321883Z","iopub.status.idle":"2024-12-02T23:44:46.364498Z","shell.execute_reply.started":"2024-12-02T23:10:23.321844Z","shell.execute_reply":"2024-12-02T23:44:46.363471Z"}},"outputs":[{"name":"stdout","text":"Unique values in Exited: [0 1]\nNumber of NaN values before cleaning:\nSurname            0\nCreditScore        0\nGeography          0\nGender             0\nAge                0\nTenure             0\nBalance            0\nNumOfProducts      0\nHasCrCard          0\nIsActiveMember     0\nEstimatedSalary    0\nExited             0\ndtype: int64\n\nNumber of NaN values after cleaning:\nSurname            0\nCreditScore        0\nGeography          0\nGender             0\nAge                0\nTenure             0\nBalance            0\nNumOfProducts      0\nHasCrCard          0\nIsActiveMember     0\nEstimatedSalary    0\nExited             0\ndtype: int64\n\nShape of transformed dataframe: (275058, 69)\n\nFirst few rows of the transformed dataframe:\n   CreditScore   Age  Tenure    Balance  NumOfProducts  HasCrCard  \\\n0        668.0  33.0     3.0       0.00            2.0        1.0   \n1        627.0  33.0     1.0       0.00            2.0        1.0   \n2        678.0  40.0    10.0       0.00            2.0        1.0   \n3        581.0  34.0     2.0  148882.54            1.0        1.0   \n4        716.0  33.0     5.0       0.00            2.0        1.0   \n\n   IsActiveMember  EstimatedSalary  Surname_hash_0  Surname_hash_1  ...  \\\n0             0.0        181449.97             0.0             0.0  ...   \n1             1.0         49503.50             0.0             0.0  ...   \n2             0.0        184866.69             0.0             0.0  ...   \n3             1.0         84560.88             0.0             0.0  ...   \n4             1.0         15068.83             0.0             0.0  ...   \n\n   Gender_hash_11  Gender_hash_12  Gender_hash_13  Gender_hash_14  \\\n0             0.0             0.0             0.0             0.0   \n1             0.0             0.0             0.0             0.0   \n2             0.0             0.0             0.0             0.0   \n3             0.0             0.0             0.0             0.0   \n4             0.0             0.0             0.0             0.0   \n\n   Gender_hash_15  Gender_hash_16  Gender_hash_17  Gender_hash_18  \\\n0             0.0             0.0             0.0             0.0   \n1             0.0             0.0             0.0             0.0   \n2             0.0             0.0             0.0             0.0   \n3             0.0             0.0             0.0             0.0   \n4             0.0             0.0             0.0             0.0   \n\n   Gender_hash_19  Exited  \n0             0.0       0  \n1             0.0       0  \n2             0.0       0  \n3             0.0       0  \n4             0.0       0  \n\n[5 rows x 69 columns]\n\nTraining SVM...\n\nSVM Results:\nAccuracy: 0.9059114375045445\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.92      0.97      0.94     44639\n           1       0.84      0.61      0.71     10373\n\n    accuracy                           0.91     55012\n   macro avg       0.88      0.79      0.83     55012\nweighted avg       0.90      0.91      0.90     55012\n\n\nTraining Random Forest...\n\nRandom Forest Results:\nAccuracy: 0.911855595142878\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.97      0.95     44639\n           1       0.83      0.67      0.74     10373\n\n    accuracy                           0.91     55012\n   macro avg       0.88      0.82      0.84     55012\nweighted avg       0.91      0.91      0.91     55012\n\n\nTraining Logistic Regression...\n\nLogistic Regression Results:\nAccuracy: 0.8672289682251145\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.88      0.96      0.92     44639\n           1       0.74      0.46      0.56     10373\n\n    accuracy                           0.87     55012\n   macro avg       0.81      0.71      0.74     55012\nweighted avg       0.86      0.87      0.85     55012\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.preprocessing import LabelEncoder\n\ndf = pd.read_csv('/kaggle/input/complete-dataset/augmented_data.csv')\ndf = df.drop_duplicates()\n\ndef fill_missing_values(df):\n    for col in df.select_dtypes(include=['float64', 'int64']).columns:\n        df[col] = df[col].fillna(df[col].mean())\n    for col in df.select_dtypes(include=['object']).columns:\n        df[col] = df[col].fillna(df[col].mode()[0])\n    return df\n\ndf = fill_missing_values(df)\n\nprint(\"Original dataset size:\", len(df))\n\ndf['Exited'] = df['Exited'].astype(int)\ndf['Exited'] = (df['Exited'] > 0).astype(int)\n\nsubsample_size = 0.3\ndf_subsampled = df.sample(frac=subsample_size, random_state=42)\nprint(\"Subsampled dataset size:\", len(df_subsampled))\n\ncolumns_to_drop = ['id', 'CustomerId']\ndf_subsampled = df_subsampled.drop(columns=columns_to_drop)\n\nle = LabelEncoder()\ncategorical_columns = ['Surname', 'Geography', 'Gender']\nfor col in categorical_columns:\n    df_subsampled[col] = le.fit_transform(df_subsampled[col].astype(str))\n\nX = df_subsampled.drop('Exited', axis=1)\ny = df_subsampled['Exited']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Train and evaluate initial models\nprint(\"\\nInitial Model Training:\")\n\n# SVM\nprint(\"\\nTraining SVM...\")\nsvm = SVC(kernel='rbf', random_state=42)\nsvm.fit(X_train_scaled, y_train)\nsvm_pred = svm.predict(X_test_scaled)\nprint(\"\\nSVM Results:\")\nprint(\"Accuracy:\", accuracy_score(y_test, svm_pred))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, svm_pred))\n\n# Random Forest\nprint(\"\\nTraining Random Forest...\")\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train_scaled, y_train)\nrf_pred = rf.predict(X_test_scaled)\nprint(\"\\nRandom Forest Results:\")\nprint(\"Accuracy:\", accuracy_score(y_test, rf_pred))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, rf_pred))\n\n# Logistic Regression\nprint(\"\\nTraining Logistic Regression...\")\nlr = LogisticRegression(random_state=42, max_iter=1000)\nlr.fit(X_train_scaled, y_train)\nlr_pred = lr.predict(X_test_scaled)\nprint(\"\\nLogistic Regression Results:\")\nprint(\"Accuracy:\", accuracy_score(y_test, lr_pred))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, lr_pred))\n\n# Create DataFrame with initial results\nresults = {\n    'Model': ['SVM', 'Random Forest', 'Logistic Regression'],\n    'Accuracy': [\n        accuracy_score(y_test, svm_pred),\n        accuracy_score(y_test, rf_pred),\n        accuracy_score(y_test, lr_pred)\n    ]\n}\n\nresults_df = pd.DataFrame(results)\nprint(\"\\nModel Comparison:\")\nprint(results_df)\n\n# Function to process data and train models\ndef process_and_train(data, sample_size):\n    # Subsample the data\n    df_sub = data.sample(frac=sample_size, random_state=42)\n    \n    # Drop unnecessary columns\n    df_sub = df_sub.drop(columns=columns_to_drop)\n    \n    # Handle categorical variables\n    for col in categorical_columns:\n        df_sub[col] = le.fit_transform(df_sub[col].astype(str))\n    \n    # Prepare features and target\n    X = df_sub.drop('Exited', axis=1)\n    y = df_sub['Exited']\n    \n    # Split and scale\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    return X_train_scaled, X_test_scaled, y_train, y_test\n\n# Test different sample sizes\nsubsample_sizes = [0.1, 0.3, 0.5]\nresults_by_size = []\n\nfor size in subsample_sizes:\n    print(f\"\\nTesting with {size*100}% of data\")\n    \n    X_train_scaled, X_test_scaled, y_train, y_test = process_and_train(df, size)\n    \n    models = {\n        'SVM': SVC(kernel='rbf', random_state=42),\n        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n    }\n    \n    for model_name, model in models.items():\n        print(f\"Training {model_name}...\")\n        model.fit(X_train_scaled, y_train)\n        pred = model.predict(X_test_scaled)\n        accuracy = accuracy_score(y_test, pred)\n        print(f\"{model_name} Accuracy: {accuracy:.4f}\")\n        \n        results_by_size.append({\n            'Sample Size': f\"{size*100}%\",\n            'Model': model_name,\n            'Accuracy': accuracy\n        })\n\n# Create DataFrame with results for different sample sizes\nresults_by_size_df = pd.DataFrame(results_by_size)\nprint(\"\\nResults for different sample sizes:\")\nprint(results_by_size_df.pivot(index='Model', columns='Sample Size', values='Accuracy'))\n\n# Save results\nresults_by_size_df.to_csv('subsampling_results.csv', index=False)\n\n# Feature importance analysis for Random Forest\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train_scaled, y_train)\nfeature_importance = pd.DataFrame({\n    'feature': X.columns,\n    'importance': rf.feature_importances_\n})\nfeature_importance = feature_importance.sort_values('importance', ascending=False)\nprint(\"\\nFeature Importance:\")\nprint(feature_importance)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T08:04:49.070038Z","iopub.execute_input":"2024-12-03T08:04:49.070296Z","iopub.status.idle":"2024-12-03T08:11:22.262072Z","shell.execute_reply.started":"2024-12-03T08:04:49.070270Z","shell.execute_reply":"2024-12-03T08:11:22.261188Z"}},"outputs":[{"name":"stdout","text":"Original dataset size: 275058\nSubsampled dataset size: 82517\n\nInitial Model Training:\n\nTraining SVM...\n\nSVM Results:\nAccuracy: 0.9111730489578284\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.92      0.98      0.95     13472\n           1       0.86      0.62      0.72      3032\n\n    accuracy                           0.91     16504\n   macro avg       0.89      0.80      0.83     16504\nweighted avg       0.91      0.91      0.91     16504\n\n\nTraining Random Forest...\n\nRandom Forest Results:\nAccuracy: 0.9134755210857974\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.97      0.95     13472\n           1       0.83      0.67      0.74      3032\n\n    accuracy                           0.91     16504\n   macro avg       0.88      0.82      0.84     16504\nweighted avg       0.91      0.91      0.91     16504\n\n\nTraining Logistic Regression...\n\nLogistic Regression Results:\nAccuracy: 0.8603974793989336\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.88      0.96      0.92     13472\n           1       0.71      0.40      0.52      3032\n\n    accuracy                           0.86     16504\n   macro avg       0.79      0.68      0.72     16504\nweighted avg       0.85      0.86      0.84     16504\n\n\nModel Comparison:\n                 Model  Accuracy\n0                  SVM  0.911173\n1        Random Forest  0.913476\n2  Logistic Regression  0.860397\n\nTesting with 10.0% of data\nTraining SVM...\nSVM Accuracy: 0.9046\nTraining Random Forest...\nRandom Forest Accuracy: 0.9091\nTraining Logistic Regression...\nLogistic Regression Accuracy: 0.8606\n\nTesting with 30.0% of data\nTraining SVM...\nSVM Accuracy: 0.9112\nTraining Random Forest...\nRandom Forest Accuracy: 0.9135\nTraining Logistic Regression...\nLogistic Regression Accuracy: 0.8604\n\nTesting with 50.0% of data\nTraining SVM...\nSVM Accuracy: 0.9098\nTraining Random Forest...\nRandom Forest Accuracy: 0.9140\nTraining Logistic Regression...\nLogistic Regression Accuracy: 0.8586\n\nResults for different sample sizes:\nSample Size             10.0%     30.0%     50.0%\nModel                                            \nLogistic Regression  0.860596  0.860397  0.858613\nRandom Forest        0.909124  0.913476  0.913982\nSVM                  0.904580  0.911173  0.909801\n\nFeature Importance:\n            feature  importance\n4               Age    0.265649\n7     NumOfProducts    0.174762\n10  EstimatedSalary    0.101464\n0           Surname    0.096259\n1       CreditScore    0.095151\n6           Balance    0.086339\n9    IsActiveMember    0.058540\n5            Tenure    0.050915\n2         Geography    0.037068\n3            Gender    0.022940\n8         HasCrCard    0.010913\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}