{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":65711,"databundleVersionId":7405009,"sourceType":"competition"},{"sourceId":9789868,"sourceType":"datasetVersion","datasetId":5998716}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import HashingVectorizer\n\ndef apply_feature_hashing(df, categorical_columns, n_features=5):\n    hasher = HashingVectorizer(n_features=n_features, norm=None, alternate_sign=False)\n    for col in categorical_columns:\n        # Ensure the column is treated as string\n        df[col] = df[col].astype(str)\n        # Apply feature hashing\n        hashed_features = hasher.transform(df[col]).toarray()\n        # Create hashed feature column names\n        hashed_columns = [f\"{col}_hash_{i}\" for i in range(n_features)]\n        hashed_df = pd.DataFrame(hashed_features, columns=hashed_columns, index=df.index)\n        # Replace the original column with hashed features\n        df = pd.concat([df.drop(columns=[col]), hashed_df], axis=1)\n    return df\n\ncategorical_columns = ['Surname', 'Geography', 'Gender']\n\n\nhashed_df = apply_feature_hashing(df, categorical_columns, n_features=5)\n\nprint(hashed_df.head())\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T23:10:21.457033Z","iopub.execute_input":"2024-12-02T23:10:21.457653Z","iopub.status.idle":"2024-12-02T23:10:23.320015Z","shell.execute_reply.started":"2024-12-02T23:10:21.457608Z","shell.execute_reply":"2024-12-02T23:10:23.319133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction import FeatureHasher\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nimport xgboost as xgb\n\n# Load and preprocess data\ndf = pd.read_csv('/kaggle/input/complete-dataset/augmented_data.csv')\ndf = df.drop_duplicates()\n\ndef fill_missing_values(df):\n    # For numerical columns\n    numerical_cols = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n    for col in numerical_cols:\n        df[col] = df[col].fillna(df[col].mean())\n    \n    # For categorical columns\n    categorical_cols = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember']\n    for col in categorical_cols:\n        df[col] = df[col].fillna(df[col].mode()[0])\n    \n    return df\n\ndf = fill_missing_values(df)\n\n# Drop unnecessary columns\ncolumns_to_drop = ['id', 'Surname', 'CustomerId']\ndf = df.drop(columns=columns_to_drop)\n\n# Convert HasCrCard and IsActiveMember to categorical\ndf['HasCrCard'] = df['HasCrCard'].astype('category')\ndf['IsActiveMember'] = df['IsActiveMember'].astype('category')\n\n# Handle missing values in target variable and convert to integer\ndf['Exited'] = df['Exited'].fillna(df['Exited'].mode()[0])\ndf['Exited'] = df['Exited'].astype(int)\n\n# Define columns\ncategorical_columns = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember']\nnumerical_columns = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n\n# Apply Feature Hashing\nhasher = FeatureHasher(n_features=10, input_type='string')\nhashed_features = []\n\nfor col in categorical_columns:\n    # Convert to string and prepare data for hashing\n    col_data = [[str(val)] for val in df[col]]\n    # Apply hashing\n    hashed = hasher.transform(col_data).toarray()\n    hashed_features.append(hashed)\n\n# Combine all hashed features\nall_hashed = np.hstack(hashed_features)\n\n# Combine with numerical features\nnumerical_data = df[numerical_columns].values\nX = np.hstack([numerical_data, all_hashed])\ny = df['Exited'].values\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\ndef evaluate_model(model, X_train_scaled, X_test_scaled, y_train, y_test, model_name):\n    # Start timing\n    start_time = time.time()\n    \n    # Train the model\n    model.fit(X_train_scaled, y_train)\n    \n    # Get predictions\n    train_pred = model.predict(X_train_scaled)\n    test_pred = model.predict(X_test_scaled)\n    \n    # End timing\n    end_time = time.time()\n    execution_time = end_time - start_time\n    \n    # Calculate accuracies\n    train_accuracy = accuracy_score(y_train, train_pred)\n    test_accuracy = accuracy_score(y_test, test_pred)\n    \n    print(f\"\\n{model_name} Results:\")\n    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n    print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n    print(f\"Execution Time: {execution_time:.2f} seconds\")\n    print(\"\\nTest Set Classification Report:\")\n    print(classification_report(y_test, test_pred))\n    \n    return train_accuracy, test_accuracy, execution_time\n\n# Initialize models\nmodels = {\n    'XGBoost': xgb.XGBClassifier(random_state=42),\n    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n}\n\nresults = {\n    'Model': [],\n    'Training Accuracy': [],\n    'Testing Accuracy': [],\n    'Execution Time': []\n}\n\n# Train and evaluate each model\nfor name, model in models.items():\n    print(f\"\\nEvaluating {name}...\")\n    train_acc, test_acc, exec_time = evaluate_model(model, X_train_scaled, X_test_scaled, \n                                                  y_train, y_test, name)\n    \n    results['Model'].append(name)\n    results['Training Accuracy'].append(train_acc)\n    results['Testing Accuracy'].append(test_acc)\n    results['Execution Time'].append(exec_time)\n\n# Create results DataFrame\nresults_df = pd.DataFrame(results)\nresults_df['Accuracy Difference'] = results_df['Training Accuracy'] - results_df['Testing Accuracy']\n\nprint(\"\\nFinal Results Summary:\")\nprint(results_df)\n\n# Save results\nresults_df.to_csv('feature_hashing_results.csv', index=False)\n\n# For Random Forest, we can look at feature importance\nrf_model = models['Random Forest']\nfeature_names = (numerical_columns + \n                [f'hash_feature_{i}' for i in range(all_hashed.shape[1])])\nfeature_importance = pd.DataFrame({\n    'feature': feature_names,\n    'importance': rf_model.feature_importances_\n})\nfeature_importance = feature_importance.sort_values('importance', ascending=False)\n\nprint(\"\\nTop 10 Most Important Features (Random Forest):\")\nprint(feature_importance.head(10))\n\n# For XGBoost, we can also look at feature importance\nxgb_model = models['XGBoost']\nxgb_feature_importance = pd.DataFrame({\n    'feature': feature_names,\n    'importance': xgb_model.feature_importances_\n})\nxgb_feature_importance = xgb_feature_importance.sort_values('importance', ascending=False)\n\nprint(\"\\nTop 10 Most Important Features (XGBoost):\")\nprint(xgb_feature_importance.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T15:54:56.450452Z","iopub.execute_input":"2024-12-03T15:54:56.450829Z","iopub.status.idle":"2024-12-03T15:55:37.236721Z","shell.execute_reply.started":"2024-12-03T15:54:56.450786Z","shell.execute_reply":"2024-12-03T15:55:37.235795Z"}},"outputs":[{"name":"stdout","text":"\nEvaluating XGBoost...\n\nXGBoost Results:\nTraining Accuracy: 0.9210\nTesting Accuracy: 0.9131\nExecution Time: 2.08 seconds\n\nTest Set Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.97      0.95     44639\n           1       0.83      0.68      0.75     10373\n\n    accuracy                           0.91     55012\n   macro avg       0.88      0.82      0.85     55012\nweighted avg       0.91      0.91      0.91     55012\n\n\nEvaluating Random Forest...\n\nRandom Forest Results:\nTraining Accuracy: 0.9996\nTesting Accuracy: 0.9105\nExecution Time: 31.55 seconds\n\nTest Set Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.97      0.95     44639\n           1       0.82      0.67      0.74     10373\n\n    accuracy                           0.91     55012\n   macro avg       0.87      0.82      0.84     55012\nweighted avg       0.91      0.91      0.91     55012\n\n\nEvaluating Logistic Regression...\n\nLogistic Regression Results:\nTraining Accuracy: 0.8675\nTesting Accuracy: 0.8671\nExecution Time: 0.45 seconds\n\nTest Set Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.88      0.96      0.92     44639\n           1       0.74      0.46      0.56     10373\n\n    accuracy                           0.87     55012\n   macro avg       0.81      0.71      0.74     55012\nweighted avg       0.86      0.87      0.85     55012\n\n\nFinal Results Summary:\n                 Model  Training Accuracy  Testing Accuracy  Execution Time  \\\n0              XGBoost           0.920971          0.913092        2.083506   \n1        Random Forest           0.999627          0.910529       31.549499   \n2  Logistic Regression           0.867510          0.867120        0.450502   \n\n   Accuracy Difference  \n0             0.007879  \n1             0.089099  \n2             0.000390  \n\nTop 10 Most Important Features (Random Forest):\n            feature  importance\n1               Age    0.284330\n4     NumOfProducts    0.164531\n5   EstimatedSalary    0.139351\n0       CreditScore    0.128280\n3           Balance    0.103370\n2            Tenure    0.056041\n40  hash_feature_34    0.029895\n43  hash_feature_37    0.026904\n7    hash_feature_1    0.026399\n22  hash_feature_16    0.012132\n\nTop 10 Most Important Features (XGBoost):\n            feature  importance\n4     NumOfProducts    0.486693\n40  hash_feature_34    0.163936\n1               Age    0.129036\n7    hash_feature_1    0.112804\n21  hash_feature_15    0.049661\n3           Balance    0.023194\n30  hash_feature_24    0.007048\n9    hash_feature_3    0.006252\n0       CreditScore    0.005769\n5   EstimatedSalary    0.005434\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.preprocessing import LabelEncoder\nimport xgboost as xgb\n\n# Load and preprocess data\ndf = pd.read_csv('/kaggle/input/complete-dataset/augmented_data.csv')\ndf = df.drop_duplicates()\n\ndef fill_missing_values(df):\n    # For numerical columns\n    numerical_cols = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n    for col in numerical_cols:\n        df[col] = df[col].fillna(df[col].mean())\n    \n    # For categorical columns (including HasCrCard and IsActiveMember)\n    categorical_cols = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember']\n    for col in categorical_cols:\n        df[col] = df[col].fillna(df[col].mode()[0])\n        df[col] = df[col].astype('category')  # Convert to categorical\n    \n    return df\n\nprint(\"Original dataset size:\", len(df))\n\ndef process_and_train(data, sample_size):\n    # Subsample the data\n    df_sub = data.sample(frac=sample_size, random_state=42)\n    print(f\"Subsampled dataset size for {sample_size*100}% sample: {len(df_sub)}\")\n    \n    df_sub = fill_missing_values(df_sub)\n    \n    # Drop unnecessary columns\n    columns_to_drop = ['id', 'Surname', 'CustomerId']\n    df_sub = df_sub.drop(columns=columns_to_drop)\n    \n    # Handle target variable\n    df_sub['Exited'] = df_sub['Exited'].fillna(df_sub['Exited'].mode()[0])\n    df_sub['Exited'] = df_sub['Exited'].astype(int)\n    \n    # Label encode categorical variables\n    le = LabelEncoder()\n    categorical_columns = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember']\n    for col in categorical_columns:\n        df_sub[col] = le.fit_transform(df_sub[col].astype(str))\n    \n    # Prepare features and target\n    X = df_sub.drop('Exited', axis=1)\n    y = df_sub['Exited']\n    \n    # Split and scale\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    return X_train_scaled, X_test_scaled, y_train, y_test, X.columns\n\ndef evaluate_model(model, X_train_scaled, X_test_scaled, y_train, y_test, model_name):\n    # Start timing\n    start_time = time.time()\n    \n    # Train the model\n    model.fit(X_train_scaled, y_train)\n    \n    # Predictions\n    train_pred = model.predict(X_train_scaled)\n    test_pred = model.predict(X_test_scaled)\n    \n    # End timing\n    end_time = time.time()\n    execution_time = end_time - start_time\n    \n    train_accuracy = accuracy_score(y_train, train_pred)\n    test_accuracy = accuracy_score(y_test, test_pred)\n    \n    print(f\"\\n{model_name} Results:\")\n    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n    print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n    print(f\"Execution Time: {execution_time:.2f} seconds\")\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_test, test_pred))\n    \n    return train_accuracy, test_accuracy, execution_time\n\n# Test different sample sizes\nsubsample_sizes = [0.1, 0.3, 0.5]\nresults_by_size = []\n\nfor size in subsample_sizes:\n    print(f\"\\nProcessing {size*100}% of data\")\n    \n    X_train_scaled, X_test_scaled, y_train, y_test, feature_names = process_and_train(df, size)\n    \n    models = {\n        'XGBoost': xgb.XGBClassifier(random_state=42),\n        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n    }\n    \n    for model_name, model in models.items():\n        train_acc, test_acc, exec_time = evaluate_model(\n            model, X_train_scaled, X_test_scaled, y_train, y_test, model_name\n        )\n        \n        results_by_size.append({\n            'Sample Size': f\"{size*100}%\",\n            'Model': model_name,\n            'Training Accuracy': train_acc,\n            'Testing Accuracy': test_acc,\n            'Accuracy Difference': train_acc - test_acc,\n            'Execution Time': exec_time\n        })\n        \n        # For Random Forest and XGBoost, analyze feature importance\n        if model_name in ['Random Forest', 'XGBoost']:\n            feature_importance = pd.DataFrame({\n                'feature': feature_names,\n                'importance': model.feature_importances_\n            })\n            feature_importance = feature_importance.sort_values('importance', ascending=False)\n            print(f\"\\nTop 10 Important Features for {model_name} ({size*100}% sample):\")\n            print(feature_importance.head(10))\n\n# Create final results DataFrame\nresults_df = pd.DataFrame(results_by_size)\n\n# Create pivot table for better visualization\npivot_results = results_df.pivot_table(\n    index='Model',\n    columns='Sample Size',\n    values=['Training Accuracy', 'Testing Accuracy', 'Accuracy Difference', 'Execution Time']\n)\n\nprint(\"\\nFinal Results Summary:\")\nprint(pivot_results)\n\n# Save results\nresults_df.to_csv('subsampling_results.csv', index=False)\npivot_results.to_csv('subsampling_results_pivot.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T15:56:49.868531Z","iopub.execute_input":"2024-12-03T15:56:49.868993Z","iopub.status.idle":"2024-12-03T15:57:23.130836Z","shell.execute_reply.started":"2024-12-03T15:56:49.868963Z","shell.execute_reply":"2024-12-03T15:57:23.129840Z"}},"outputs":[{"name":"stdout","text":"Original dataset size: 275058\n\nProcessing 10.0% of data\nSubsampled dataset size for 10.0% sample: 27506\n\nXGBoost Results:\nTraining Accuracy: 0.9477\nTesting Accuracy: 0.9042\nExecution Time: 0.18 seconds\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.96      0.94      4478\n           1       0.79      0.66      0.72      1024\n\n    accuracy                           0.90      5502\n   macro avg       0.86      0.81      0.83      5502\nweighted avg       0.90      0.90      0.90      5502\n\n\nTop 10 Important Features for XGBoost (10.0% sample):\n           feature  importance\n6    NumOfProducts    0.464883\n8   IsActiveMember    0.174133\n3              Age    0.119818\n2           Gender    0.073792\n1        Geography    0.060707\n5          Balance    0.030151\n9  EstimatedSalary    0.019567\n7        HasCrCard    0.019542\n0      CreditScore    0.019256\n4           Tenure    0.018150\n\nRandom Forest Results:\nTraining Accuracy: 1.0000\nTesting Accuracy: 0.9091\nExecution Time: 2.69 seconds\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.92      0.97      0.95      4478\n           1       0.82      0.66      0.73      1024\n\n    accuracy                           0.91      5502\n   macro avg       0.87      0.81      0.84      5502\nweighted avg       0.91      0.91      0.91      5502\n\n\nTop 10 Important Features for Random Forest (10.0% sample):\n           feature  importance\n3              Age    0.276783\n6    NumOfProducts    0.164268\n9  EstimatedSalary    0.128988\n0      CreditScore    0.120028\n5          Balance    0.103959\n4           Tenure    0.064941\n8   IsActiveMember    0.059983\n1        Geography    0.037173\n2           Gender    0.029122\n7        HasCrCard    0.014756\n\nLogistic Regression Results:\nTraining Accuracy: 0.8562\nTesting Accuracy: 0.8604\nExecution Time: 0.02 seconds\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.87      0.97      0.92      4478\n           1       0.74      0.39      0.51      1024\n\n    accuracy                           0.86      5502\n   macro avg       0.81      0.68      0.71      5502\nweighted avg       0.85      0.86      0.84      5502\n\n\nProcessing 30.0% of data\nSubsampled dataset size for 30.0% sample: 82517\n\nXGBoost Results:\nTraining Accuracy: 0.9291\nTesting Accuracy: 0.9131\nExecution Time: 0.46 seconds\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.97      0.95     13472\n           1       0.82      0.68      0.74      3032\n\n    accuracy                           0.91     16504\n   macro avg       0.87      0.82      0.84     16504\nweighted avg       0.91      0.91      0.91     16504\n\n\nTop 10 Important Features for XGBoost (30.0% sample):\n           feature  importance\n6    NumOfProducts    0.508814\n8   IsActiveMember    0.166671\n3              Age    0.130455\n1        Geography    0.065583\n2           Gender    0.060551\n5          Balance    0.023071\n7        HasCrCard    0.012126\n0      CreditScore    0.011315\n9  EstimatedSalary    0.011040\n4           Tenure    0.010374\n\nRandom Forest Results:\nTraining Accuracy: 0.9999\nTesting Accuracy: 0.9117\nExecution Time: 9.84 seconds\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.97      0.95     13472\n           1       0.82      0.67      0.74      3032\n\n    accuracy                           0.91     16504\n   macro avg       0.87      0.82      0.84     16504\nweighted avg       0.91      0.91      0.91     16504\n\n\nTop 10 Important Features for Random Forest (30.0% sample):\n           feature  importance\n3              Age    0.280345\n6    NumOfProducts    0.174862\n9  EstimatedSalary    0.129528\n0      CreditScore    0.119002\n5          Balance    0.100293\n4           Tenure    0.060539\n8   IsActiveMember    0.059497\n1        Geography    0.039106\n2           Gender    0.024414\n7        HasCrCard    0.012414\n\nLogistic Regression Results:\nTraining Accuracy: 0.8556\nTesting Accuracy: 0.8598\nExecution Time: 0.06 seconds\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.88      0.96      0.92     13472\n           1       0.71      0.40      0.51      3032\n\n    accuracy                           0.86     16504\n   macro avg       0.79      0.68      0.72     16504\nweighted avg       0.85      0.86      0.84     16504\n\n\nProcessing 50.0% of data\nSubsampled dataset size for 50.0% sample: 137529\n\nXGBoost Results:\nTraining Accuracy: 0.9248\nTesting Accuracy: 0.9145\nExecution Time: 0.56 seconds\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.97      0.95     22342\n           1       0.83      0.68      0.75      5164\n\n    accuracy                           0.91     27506\n   macro avg       0.88      0.83      0.85     27506\nweighted avg       0.91      0.91      0.91     27506\n\n\nTop 10 Important Features for XGBoost (50.0% sample):\n           feature  importance\n6    NumOfProducts    0.523633\n8   IsActiveMember    0.171697\n3              Age    0.124660\n1        Geography    0.067102\n2           Gender    0.053490\n5          Balance    0.023048\n0      CreditScore    0.009530\n7        HasCrCard    0.009173\n9  EstimatedSalary    0.008937\n4           Tenure    0.008731\n\nRandom Forest Results:\nTraining Accuracy: 0.9998\nTesting Accuracy: 0.9129\nExecution Time: 17.36 seconds\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.97      0.95     22342\n           1       0.83      0.67      0.74      5164\n\n    accuracy                           0.91     27506\n   macro avg       0.88      0.82      0.85     27506\nweighted avg       0.91      0.91      0.91     27506\n\n\nTop 10 Important Features for Random Forest (50.0% sample):\n           feature  importance\n3              Age    0.278518\n6    NumOfProducts    0.172878\n9  EstimatedSalary    0.132278\n0      CreditScore    0.122234\n5          Balance    0.100297\n4           Tenure    0.060059\n8   IsActiveMember    0.060011\n1        Geography    0.039220\n2           Gender    0.023042\n7        HasCrCard    0.011463\n\nLogistic Regression Results:\nTraining Accuracy: 0.8564\nTesting Accuracy: 0.8586\nExecution Time: 0.11 seconds\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.87      0.96      0.92     22342\n           1       0.72      0.40      0.52      5164\n\n    accuracy                           0.86     27506\n   macro avg       0.80      0.68      0.72     27506\nweighted avg       0.85      0.86      0.84     27506\n\n\nFinal Results Summary:\n                    Accuracy Difference                     Execution Time  \\\nSample Size                       10.0%     30.0%     50.0%          10.0%   \nModel                                                                        \nLogistic Regression           -0.004206 -0.004233 -0.002192       0.021367   \nRandom Forest                  0.090831  0.088145  0.086981       2.685314   \nXGBoost                        0.043520  0.016023  0.010333       0.182222   \n\n                                         Testing Accuracy                      \\\nSample Size             30.0%      50.0%            10.0%     30.0%     50.0%   \nModel                                                                           \nLogistic Regression  0.061760   0.105245         0.860414  0.859792  0.858613   \nRandom Forest        9.842947  17.362751         0.909124  0.911718  0.912855   \nXGBoost              0.455037   0.560820         0.904217  0.913112  0.914491   \n\n                    Training Accuracy                      \nSample Size                     10.0%     30.0%     50.0%  \nModel                                                      \nLogistic Regression          0.856208  0.855559  0.856421  \nRandom Forest                0.999955  0.999864  0.999836  \nXGBoost                      0.947737  0.929135  0.924825  \n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}