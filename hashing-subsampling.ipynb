{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":65711,"databundleVersionId":7405009,"sourceType":"competition"},{"sourceId":9789868,"sourceType":"datasetVersion","datasetId":5998716}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import HashingVectorizer\n\ndef apply_feature_hashing(df, categorical_columns, n_features=5):\n    hasher = HashingVectorizer(n_features=n_features, norm=None, alternate_sign=False)\n    for col in categorical_columns:\n        # Ensure the column is treated as string\n        df[col] = df[col].astype(str)\n        # Apply feature hashing\n        hashed_features = hasher.transform(df[col]).toarray()\n        # Create hashed feature column names\n        hashed_columns = [f\"{col}_hash_{i}\" for i in range(n_features)]\n        hashed_df = pd.DataFrame(hashed_features, columns=hashed_columns, index=df.index)\n        # Replace the original column with hashed features\n        df = pd.concat([df.drop(columns=[col]), hashed_df], axis=1)\n    return df\n\ncategorical_columns = ['Surname', 'Geography', 'Gender']\n\n\nhashed_df = apply_feature_hashing(df, categorical_columns, n_features=5)\n\nprint(hashed_df.head())\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T23:10:21.457033Z","iopub.execute_input":"2024-12-02T23:10:21.457653Z","iopub.status.idle":"2024-12-02T23:10:23.320015Z","shell.execute_reply.started":"2024-12-02T23:10:21.457608Z","shell.execute_reply":"2024-12-02T23:10:23.319133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction import FeatureHasher\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load and preprocess data\ndf = pd.read_csv('/kaggle/input/complete-dataset/augmented_data.csv')\ndf = df.drop_duplicates()\n\ndef fill_missing_values(df):\n    for col in df.select_dtypes(include=['float64', 'int64']).columns:\n        df[col] = df[col].fillna(df[col].mean())\n    for col in df.select_dtypes(include=['object']).columns:\n        df[col] = df[col].fillna(df[col].mode()[0])\n    return df\n\ndf = fill_missing_values(df)\n\n# Drop unnecessary columns\ncolumns_to_drop = ['id', 'CustomerId']\ndf = df.drop(columns=columns_to_drop)\n\n# Convert target variable\ndf['Exited'] = df['Exited'].astype(int)\n\n# Define columns\ncategorical_columns = ['Surname', 'Geography', 'Gender']\nnumerical_columns = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', \n                    'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n\n# Apply Feature Hashing\nhasher = FeatureHasher(n_features=10, input_type='string')  # Reduced n_features for example\nhashed_features = []\n\nfor col in categorical_columns:\n    # Prepare data for hashing\n    col_data = [[str(val)] for val in df[col]]\n    # Apply hashing\n    hashed = hasher.transform(col_data).toarray()\n    hashed_features.append(hashed)\n\n# Combine all hashed features\nall_hashed = np.hstack(hashed_features)\n\n# Combine with numerical features\nnumerical_data = df[numerical_columns].values\nX = np.hstack([numerical_data, all_hashed])\ny = df['Exited'].values\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\ndef evaluate_model(model, X_train_scaled, X_test_scaled, y_train, y_test, model_name):\n    # Train the model\n    model.fit(X_train_scaled, y_train)\n    \n    # Get predictions\n    train_pred = model.predict(X_train_scaled)\n    test_pred = model.predict(X_test_scaled)\n    \n    # Calculate accuracies\n    train_accuracy = accuracy_score(y_train, train_pred)\n    test_accuracy = accuracy_score(y_test, test_pred)\n    \n    print(f\"\\n{model_name} Results:\")\n    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n    print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n    print(\"\\nTest Set Classification Report:\")\n    print(classification_report(y_test, test_pred))\n    \n    return train_accuracy, test_accuracy\n\n# Initialize and evaluate models\nmodels = {\n    'SVM': SVC(kernel='rbf', random_state=42),\n    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n}\n\nresults = {\n    'Model': [],\n    'Training Accuracy': [],\n    'Testing Accuracy': []\n}\n\n# Train and evaluate each model\nfor name, model in models.items():\n    print(f\"\\nEvaluating {name}...\")\n    train_acc, test_acc = evaluate_model(model, X_train_scaled, X_test_scaled, \n                                       y_train, y_test, name)\n    \n    results['Model'].append(name)\n    results['Training Accuracy'].append(train_acc)\n    results['Testing Accuracy'].append(test_acc)\n\n# Create results DataFrame\nresults_df = pd.DataFrame(results)\nresults_df['Accuracy Difference'] = results_df['Training Accuracy'] - results_df['Testing Accuracy']\n\nprint(\"\\nFinal Results Summary:\")\nprint(results_df)\n\n# Save results\nresults_df.to_csv('feature_hashing_results.csv', index=False)\n\n# For Random Forest, we can also look at feature importance\nrf_model = models['Random Forest']\nfeature_names = (numerical_columns + \n                [f'hash_feature_{i}' for i in range(all_hashed.shape[1])])\nfeature_importance = pd.DataFrame({\n    'feature': feature_names,\n    'importance': rf_model.feature_importances_\n})\nfeature_importance = feature_importance.sort_values('importance', ascending=False)\n\nprint(\"\\nTop 10 Most Important Features:\")\nprint(feature_importance.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T08:52:51.116839Z","iopub.execute_input":"2024-12-03T08:52:51.117761Z","iopub.status.idle":"2024-12-03T09:33:28.909540Z","shell.execute_reply.started":"2024-12-03T08:52:51.117726Z","shell.execute_reply":"2024-12-03T09:33:28.908603Z"}},"outputs":[{"name":"stdout","text":"\nEvaluating SVM...\n\nSVM Results:\nTraining Accuracy: 0.9105\nTesting Accuracy: 0.9072\n\nTest Set Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.92      0.97      0.94     44639\n           1       0.85      0.62      0.72     10373\n\n    accuracy                           0.91     55012\n   macro avg       0.88      0.80      0.83     55012\nweighted avg       0.90      0.91      0.90     55012\n\n\nEvaluating Random Forest...\n\nRandom Forest Results:\nTraining Accuracy: 0.9998\nTesting Accuracy: 0.9118\n\nTest Set Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.97      0.95     44639\n           1       0.83      0.67      0.74     10373\n\n    accuracy                           0.91     55012\n   macro avg       0.88      0.82      0.84     55012\nweighted avg       0.91      0.91      0.91     55012\n\n\nEvaluating Logistic Regression...\n\nLogistic Regression Results:\nTraining Accuracy: 0.8676\nTesting Accuracy: 0.8672\n\nTest Set Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.88      0.96      0.92     44639\n           1       0.74      0.46      0.56     10373\n\n    accuracy                           0.87     55012\n   macro avg       0.81      0.71      0.74     55012\nweighted avg       0.86      0.87      0.85     55012\n\n\nFinal Results Summary:\n                 Model  Training Accuracy  Testing Accuracy  \\\n0                  SVM           0.910482          0.907220   \n1        Random Forest           0.999805          0.911801   \n2  Logistic Regression           0.867564          0.867174   \n\n   Accuracy Difference  \n0             0.003262  \n1             0.088004  \n2             0.000390  \n\nTop 10 Most Important Features:\n            feature  importance\n1               Age    0.269811\n4     NumOfProducts    0.163200\n7   EstimatedSalary    0.103897\n0       CreditScore    0.099452\n3           Balance    0.086930\n6    IsActiveMember    0.057175\n2            Tenure    0.055489\n19  hash_feature_11    0.027538\n33  hash_feature_25    0.011907\n34  hash_feature_26    0.011419\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.preprocessing import LabelEncoder\n\ndf = pd.read_csv('/kaggle/input/complete-dataset/augmented_data.csv')\ndf = df.drop_duplicates()\n\ndef fill_missing_values(df):\n    for col in df.select_dtypes(include=['float64', 'int64']).columns:\n        df[col] = df[col].fillna(df[col].mean())\n    for col in df.select_dtypes(include=['object']).columns:\n        df[col] = df[col].fillna(df[col].mode()[0])\n    return df\n\ndf = fill_missing_values(df)\n\nprint(\"Original dataset size:\", len(df))\n\ndf['Exited'] = df['Exited'].astype(int)\ndf['Exited'] = (df['Exited'] > 0).astype(int)\n\nsubsample_size = 0.3\ndf_subsampled = df.sample(frac=subsample_size, random_state=42)\nprint(\"Subsampled dataset size:\", len(df_subsampled))\n\ncolumns_to_drop = ['id', 'CustomerId']\ndf_subsampled = df_subsampled.drop(columns=columns_to_drop)\n\nle = LabelEncoder()\ncategorical_columns = ['Surname', 'Geography', 'Gender']\nfor col in categorical_columns:\n    df_subsampled[col] = le.fit_transform(df_subsampled[col].astype(str))\n\nX = df_subsampled.drop('Exited', axis=1)\ny = df_subsampled['Exited']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Train and evaluate initial models\nprint(\"\\nInitial Model Training:\")\n\n# SVM\nprint(\"\\nTraining SVM...\")\nsvm = SVC(kernel='rbf', random_state=42)\nsvm.fit(X_train_scaled, y_train)\nsvm_pred = svm.predict(X_test_scaled)\nprint(\"\\nSVM Results:\")\nprint(\"Accuracy:\", accuracy_score(y_test, svm_pred))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, svm_pred))\n\n# Random Forest\nprint(\"\\nTraining Random Forest...\")\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train_scaled, y_train)\nrf_pred = rf.predict(X_test_scaled)\nprint(\"\\nRandom Forest Results:\")\nprint(\"Accuracy:\", accuracy_score(y_test, rf_pred))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, rf_pred))\n\n# Logistic Regression\nprint(\"\\nTraining Logistic Regression...\")\nlr = LogisticRegression(random_state=42, max_iter=1000)\nlr.fit(X_train_scaled, y_train)\nlr_pred = lr.predict(X_test_scaled)\nprint(\"\\nLogistic Regression Results:\")\nprint(\"Accuracy:\", accuracy_score(y_test, lr_pred))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, lr_pred))\n\n# Create DataFrame with initial results\nresults = {\n    'Model': ['SVM', 'Random Forest', 'Logistic Regression'],\n    'Accuracy': [\n        accuracy_score(y_test, svm_pred),\n        accuracy_score(y_test, rf_pred),\n        accuracy_score(y_test, lr_pred)\n    ]\n}\n\nresults_df = pd.DataFrame(results)\nprint(\"\\nModel Comparison:\")\nprint(results_df)\n\n# Function to process data and train models\ndef process_and_train(data, sample_size):\n    # Subsample the data\n    df_sub = data.sample(frac=sample_size, random_state=42)\n    \n    # Drop unnecessary columns\n    df_sub = df_sub.drop(columns=columns_to_drop)\n    \n    # Handle categorical variables\n    for col in categorical_columns:\n        df_sub[col] = le.fit_transform(df_sub[col].astype(str))\n    \n    # Prepare features and target\n    X = df_sub.drop('Exited', axis=1)\n    y = df_sub['Exited']\n    \n    # Split and scale\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    return X_train_scaled, X_test_scaled, y_train, y_test\n\n# Test different sample sizes\nsubsample_sizes = [0.1, 0.3, 0.5]\nresults_by_size = []\n\nfor size in subsample_sizes:\n    print(f\"\\nTesting with {size*100}% of data\")\n    \n    X_train_scaled, X_test_scaled, y_train, y_test = process_and_train(df, size)\n    \n    models = {\n        'SVM': SVC(kernel='rbf', random_state=42),\n        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n    }\n    \n    for model_name, model in models.items():\n        print(f\"Training {model_name}...\")\n        model.fit(X_train_scaled, y_train)\n        pred = model.predict(X_test_scaled)\n        accuracy = accuracy_score(y_test, pred)\n        print(f\"{model_name} Accuracy: {accuracy:.4f}\")\n        \n        results_by_size.append({\n            'Sample Size': f\"{size*100}%\",\n            'Model': model_name,\n            'Accuracy': accuracy\n        })\n\n# Create DataFrame with results for different sample sizes\nresults_by_size_df = pd.DataFrame(results_by_size)\nprint(\"\\nResults for different sample sizes:\")\nprint(results_by_size_df.pivot(index='Model', columns='Sample Size', values='Accuracy'))\n\n# Save results\nresults_by_size_df.to_csv('subsampling_results.csv', index=False)\n\n# Feature importance analysis for Random Forest\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train_scaled, y_train)\nfeature_importance = pd.DataFrame({\n    'feature': X.columns,\n    'importance': rf.feature_importances_\n})\nfeature_importance = feature_importance.sort_values('importance', ascending=False)\nprint(\"\\nFeature Importance:\")\nprint(feature_importance)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T09:40:42.300621Z","iopub.execute_input":"2024-12-03T09:40:42.301036Z","iopub.status.idle":"2024-12-03T09:47:17.670266Z","shell.execute_reply.started":"2024-12-03T09:40:42.301008Z","shell.execute_reply":"2024-12-03T09:47:17.669357Z"}},"outputs":[{"name":"stdout","text":"Original dataset size: 275058\nSubsampled dataset size: 82517\n\nInitial Model Training:\n\nTraining SVM...\n\nSVM Results:\nAccuracy: 0.9111730489578284\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.92      0.98      0.95     13472\n           1       0.86      0.62      0.72      3032\n\n    accuracy                           0.91     16504\n   macro avg       0.89      0.80      0.83     16504\nweighted avg       0.91      0.91      0.91     16504\n\n\nTraining Random Forest...\n\nRandom Forest Results:\nAccuracy: 0.9134755210857974\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.93      0.97      0.95     13472\n           1       0.83      0.67      0.74      3032\n\n    accuracy                           0.91     16504\n   macro avg       0.88      0.82      0.84     16504\nweighted avg       0.91      0.91      0.91     16504\n\n\nTraining Logistic Regression...\n\nLogistic Regression Results:\nAccuracy: 0.8603974793989336\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.88      0.96      0.92     13472\n           1       0.71      0.40      0.52      3032\n\n    accuracy                           0.86     16504\n   macro avg       0.79      0.68      0.72     16504\nweighted avg       0.85      0.86      0.84     16504\n\n\nModel Comparison:\n                 Model  Accuracy\n0                  SVM  0.911173\n1        Random Forest  0.913476\n2  Logistic Regression  0.860397\n\nTesting with 10.0% of data\nTraining SVM...\nSVM Accuracy: 0.9046\nTraining Random Forest...\nRandom Forest Accuracy: 0.9091\nTraining Logistic Regression...\nLogistic Regression Accuracy: 0.8606\n\nTesting with 30.0% of data\nTraining SVM...\nSVM Accuracy: 0.9112\nTraining Random Forest...\nRandom Forest Accuracy: 0.9135\nTraining Logistic Regression...\nLogistic Regression Accuracy: 0.8604\n\nTesting with 50.0% of data\nTraining SVM...\nSVM Accuracy: 0.9098\nTraining Random Forest...\nRandom Forest Accuracy: 0.9140\nTraining Logistic Regression...\nLogistic Regression Accuracy: 0.8586\n\nResults for different sample sizes:\nSample Size             10.0%     30.0%     50.0%\nModel                                            \nLogistic Regression  0.860596  0.860397  0.858613\nRandom Forest        0.909124  0.913476  0.913982\nSVM                  0.904580  0.911173  0.909801\n\nFeature Importance:\n            feature  importance\n4               Age    0.265649\n7     NumOfProducts    0.174762\n10  EstimatedSalary    0.101464\n0           Surname    0.096259\n1       CreditScore    0.095151\n6           Balance    0.086339\n9    IsActiveMember    0.058540\n5            Tenure    0.050915\n2         Geography    0.037068\n3            Gender    0.022940\n8         HasCrCard    0.010913\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}